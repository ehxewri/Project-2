{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Set Up Kaggle and API Key\n",
    "\n",
    "Follow these steps to install and configure the Kaggle API on your system:\n",
    "\n",
    "1. **Create a Kaggle Account**\n",
    "   - Visit [Kaggle](https://www.kaggle.com) and sign up for an account.\n",
    "\n",
    "2. **Obtain Kaggle API Key**\n",
    "   - Go to your Kaggle account settings.\n",
    "   - Find the \"API\" section and click on \"Create New API Token\".\n",
    "   - This will download a `kaggle.json` file containing your API key.\n",
    "\n",
    "3. **Install Kaggle Package**\n",
    "   - Use Conda to install the Kaggle package by running:\n",
    "     ```bash\n",
    "     conda install kaggle\n",
    "     ```\n",
    "\n",
    "4. **Configure API Key**\n",
    "   - Copy the `kaggle.json` file to your user directory under the `.kaggle` folder. On most systems, you can use the following command:\n",
    "     ```bash\n",
    "     mkdir -p ~/.kaggle\n",
    "     cp path_to_downloaded_kaggle.json ~/.kaggle/kaggle.json\n",
    "     chmod 600 ~/.kaggle/kaggle.json\n",
    "     ```\n",
    "   - Ensure the `.kaggle` directory and the `kaggle.json` file have the proper permissions by setting:\n",
    "     ```bash\n",
    "     chmod 600 ~/.kaggle/kaggle.json\n",
    "     ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import kaggle\n",
    "# Pre processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Scoring \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# models \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data using an API call\n",
    "kaggle.api.dataset_download_files('rodsaldanha/arketing-campaign', path='resources', unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv(\"./resources/marketing_campaign.csv\",delimiter=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis)\n",
    "We will revisit this. For now We want the rough draft of the model\n",
    "#\n",
    "During EDA\n",
    "\n",
    "Visualize the data using plots and graphs to understand distributions and relationships between variables.\n",
    "Calculate summary statistics to get a sense of the central tendencies and variability.\n",
    "Identify any correlations between variables that might influence model choices.\n",
    "Detect and treat missing values or outliers that could skew the results of your analysis.\n",
    "Explore the data's structure to inform feature selection and engineering, which are key to building effective machine learning models.\n",
    "\n",
    "# read any and all documentation you can find on your dataset to understand it better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "display (data.head())\n",
    "# what does our data look like? At this point also use any documentation on the data set to find out what each value means and how it might be used is solving the business problem\n",
    "display (data.shape)\n",
    "print (f'Columns with NA valuses \\n {data.isna().sum()[lambda x: x > 0]}')\n",
    "# Make desision about null values. Can we fill them of should we drop rows with null values?\n",
    "non_numeric= (data.dtypes[(data.dtypes != 'int64') & (data.dtypes != 'float64')]).index.tolist()\n",
    "# display (data.dtypes)\n",
    "print (f'Columns that are not numeric :\\n {non_numeric}')\n",
    "# Explore non numberic type to see how we can use them in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to get started we will drop NA and columns that are not numberic. this will let us get a rough model\n",
    "# we come back to this and preprocess based on the draft results if needed\n",
    "\n",
    "\n",
    "data_drop_columns = data.drop(columns=non_numeric, axis=1)\n",
    "data_drop_na = data_drop_columns.dropna()\n",
    "df = data_drop_na.copy()\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Train and Test **80/20 split**\n",
    "# add verbage as to why we picked response\n",
    "\n",
    "X = df.drop('Response', axis=1)\n",
    "y = df[\"Response\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "# This will split 'X' and 'y' such that 80% is used for training and 20% is used for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the data \n",
    "We will want to compare the scores of standard scalar to Min Max scalar to pick the bast scaling methood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.28465542,  1.67814135, -1.34041544, ..., -0.0923974 ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.33894248, -1.62391459,  0.41722354, ..., -0.0923974 ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.54518097, -0.38564361, -0.37026134, ..., -0.0923974 ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.19420618,  0.43987037, -1.19440407, ..., -0.0923974 ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.40128059,  0.60497317,  0.55792611, ..., -0.0923974 ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.63378252,  0.43987037,  0.59547522, ..., -0.0923974 ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale the X data by using StandardScaler()\n",
    "scaler_ss = StandardScaler().fit(X_train)\n",
    "X_train_ss_scaled = scaler_ss.transform(X_train)\n",
    "X_train_ss_scaled\n",
    "\n",
    "# Transform the test dataset based on the fit from the training dataset\n",
    "X_test_ss_scaled = scaler_ss.transform(X_test)\n",
    "X_test_ss_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46054866, 0.76699029, 0.05418567, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.40988294, 0.49514563, 0.06508296, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.17219194, 0.77669903, 0.1017707 , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.02198195, 0.69902913, 0.09737779, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.19444196, 0.76699029, 0.1179121 , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.96792065, 0.63106796, 0.06749522, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5822536 , 0.93203883, 0.02369702, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.40130462, 0.54368932, 0.0919111 , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.05129122, 0.68932039, 0.06134876, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.44330265, 0.78640777, 0.02936373, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.90626396, 0.80582524, 0.09737178, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.97372889, 0.78640777, 0.09882906, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now lets look at min max scaler\n",
    "scaler_mm = MinMaxScaler().fit(X_train)\n",
    "X_train_mm_scaled = scaler_mm.transform(X_train)\n",
    "display (X_train_mm_scaled)\n",
    "#\n",
    "X_test_mm_scaled = scaler_mm.transform(X_test)\n",
    "display (X_test_mm_scaled)\n",
    "\n",
    "X_test_mm_scaled = scaler_mm.transform(X_test)\n",
    "display (X_test_mm_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Logistic model to find out what scaler works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Scaler\n",
      "Training Data Score: 0.8871331828442438\n",
      "Testing Data Score: 0.8738738738738738\n",
      "Min Max Scaler\n",
      "Training Data Score: 0.8820541760722348\n",
      "Testing Data Score: 0.8986486486486487\n"
     ]
    }
   ],
   "source": [
    "# Create a `LogisticRegression` function and assign it \n",
    "# to a variable named `logistic_regression_model`.\n",
    "logistic_regression_model_ss = LogisticRegression()\n",
    "logistic_regression_model_ss.fit(X_train_ss_scaled, y_train)\n",
    "#\n",
    "logistic_regression_model_mm = LogisticRegression()\n",
    "logistic_regression_model_mm.fit(X_train_mm_scaled, y_train)\n",
    "# Score the Logistic model\n",
    "\n",
    "print(f\"Standard Scaler\\nTraining Data Score: {logistic_regression_model_ss.score(X_train_ss_scaled, y_train)}\")\n",
    "print(f\"Testing Data Score: {logistic_regression_model_ss.score(X_test_ss_scaled, y_test)}\")\n",
    "print(f\"Min Max Scaler\\nTraining Data Score: {logistic_regression_model_mm.score(X_train_mm_scaled, y_train)}\")\n",
    "print(f\"Testing Data Score: {logistic_regression_model_mm.score(X_test_mm_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test models\n",
    "    -RANDOM FOREST MODEL\n",
    "    -GradientBoostingClassifier\n",
    "    -KNeighborsClassifier\n",
    "    -SVC (Support Vector Machine)\n",
    "    -LogisticRegression\n",
    "    -Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RANDOM FOREST MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_val_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m random_forest_model\u001b[38;5;241m.\u001b[39mpredict(X_test_ss_scaled)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate precision, recall, F1 score\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Cross-validation scores\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m(random_forest_model, X_train_ss_scaled, y_train, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Score the model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest - Training Data Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_forest_model\u001b[38;5;241m.\u001b[39mscore(X_train_ss_scaled,\u001b[38;5;250m \u001b[39my_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cross_val_score' is not defined"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "random_forest_model.fit(X_train_ss_scaled, y_train)\n",
    "# Predict on test set\n",
    "y_pred = random_forest_model.predict(X_test_ss_scaled)\n",
    "# Calculate precision, recall, F1 score\n",
    "# Cross-validation scores\n",
    "cv_scores = cross_val_score(random_forest_model, X_train_ss_scaled, y_train, cv=5, scoring='accuracy')\n",
    "# Score the model\n",
    "print(f\"Random Forest - Training Data Score: {random_forest_model.score(X_train_ss_scaled, y_train)}\")\n",
    "print(f\"Random Forest - Testing Data Score: {random_forest_model.score(X_test_ss_scaled, y_test)}\")\n",
    "print(f\"Random Forest - Precision: {precision_score(y_test, y_pred)}\")\n",
    "print(f\"Random Forest - Recall: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"Random Forest - F1 Score: {f1_score(y_test, y_pred)}\")\n",
    "print(f\"Random Forest - Cross-Validation Accuracy: {cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier MODELING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "gbm_model = GradientBoostingClassifier(random_state=42)\n",
    "gbm_model.fit(X_train_ss_scaled, y_train)\n",
    "# Predict on test set\n",
    "y_pred = gbm_model.predict(X_test_ss_scaled)\n",
    "cv_scores = cross_val_score(gbm_model, X_train_ss_scaled, y_train, cv=5, scoring='accuracy')\n",
    "# Score the model\n",
    "print(f\"Gradient Boosting Machine - Training Data Score: {gbm_model.score(X_train_ss_scaled, y_train)}\")\n",
    "print(f\"Gradient Boosting Machine - Testing Data Score: {gbm_model.score(X_test_ss_scaled, y_test)}\")\n",
    "print(f\"Gradient Boosting Machine - Precision: {precision_score(y_test, y_pred)}\")\n",
    "print(f\"Gradient Boosting Machine - Recall: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"Gradient Boosting Machine - F1 Score: {f1_score(y_test, y_pred)}\")\n",
    "print(f\"Gradient Boosting Machine - Cross-Validation Accuracy: {cv_scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create and train the model\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train_ss_scaled, y_train)\n",
    "# Predict on test set\n",
    "y_pred = knn_model.predict(X_test_ss_scaled)\n",
    "cv_scores = cross_val_score(knn_model, X_train_ss_scaled, y_train, cv=5, scoring='accuracy')\n",
    "# Score the model\n",
    "print(f\"K-Nearest Neighbors - Training Data Score: {knn_model.score(X_train_ss_scaled, y_train)}\")\n",
    "print(f\"K-Nearest Neighbors - Testing Data Score: {knn_model.score(X_test_ss_scaled, y_test)}\")\n",
    "print(f\"K-Nearest Neighbors - Precision: {precision_score(y_test, y_pred)}\")\n",
    "print(f\"K-Nearest Neighbors - Recall: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"K-Nearest Neighbors - F1 Score: {f1_score(y_test, y_pred)}\")\n",
    "print(f\"K-Nearest Neighbors - Cross-Validation Accuracy: {cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC (Support Vector Machine) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create and train the model\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_ss_scaled, y_train)\n",
    "# Predict on test set\n",
    "y_pred = svm_model.predict(X_test_ss_scaled)\n",
    "# Cross-validation scores\n",
    "cv_scores = cross_val_score(svm_model, X_train_ss_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Score the model\n",
    "print(f\"Support Vector Machine - Training Data Score: {svm_model.score(X_train_ss_scaled, y_train)}\")\n",
    "print(f\"Support Vector Machine - Testing Data Score: {svm_model.score(X_test_ss_scaled, y_test)}\")\n",
    "print(f\"Support Vector Machine - Precision: {precision_score(y_test, y_pred)}\")\n",
    "print(f\"Support Vector Machine - Recall: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"Support Vector Machine - F1 Score: {f1_score(y_test, y_pred)}\")\n",
    "print(f\"Support Vector Machine - Cross-Validation Accuracy: {cv_scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "logistic_regression_model = LogisticRegression()\n",
    "logistic_regression_model.fit(X_train_ss_scaled, y_train)\n",
    "# Predict on test set\n",
    "y_pred = logistic_regression_model.predict(X_test_ss_scaled)\n",
    "# Cross-validation scores\n",
    "cv_scores = cross_val_score(logistic_regression_model, X_train_ss_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Score the model\n",
    "print(f\"Logistic Regression - Training Data Score: {logistic_regression_model.score(X_train_ss_scaled, y_train)}\")\n",
    "print(f\"Logistic Regression - Testing Data Score: {logistic_regression_model.score(X_test_ss_scaled, y_test)}\")\n",
    "print(f\"Logistic Regression - Precision: {precision_score(y_test, y_pred)}\")\n",
    "print(f\"Logistic Regression - Recall: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"Logistic Regression - F1 Score: {f1_score(y_test, y_pred)}\")\n",
    "print(f\"Logistic Regression - Cross-Validation Accuracy: {cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree_model.fit(X_train_ss_scaled, y_train)\n",
    "# Predict on test set\n",
    "y_pred = decision_tree_model.predict(X_test_ss_scaled)\n",
    "# Cross-validation scores\n",
    "cv_scores = cross_val_score(decision_tree_model, X_train_ss_scaled, y_train, cv=5, scoring='accuracy')\n",
    "# Score the model\n",
    "print(f\"Decision Tree - Training Data Score: {decision_tree_model.score(X_train_ss_scaled, y_train)}\")\n",
    "print(f\"Decision Tree - Testing Data Score: {decision_tree_model.score(X_test_ss_scaled, y_test)}\")\n",
    "print(f\"Decision Tree - Precision: {precision_score(y_test, y_pred)}\")\n",
    "print(f\"Decision Tree - Recall: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"Decision Tree - F1 Score: {f1_score(y_test, y_pred)}\")\n",
    "print(f\"Decision Tree - Cross-Validation Accuracy: {cv_scores.mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
