{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Set Up Kaggle and API Key\n",
    "\n",
    "Follow these steps to install and configure the Kaggle API on your system:\n",
    "\n",
    "1. **Create a Kaggle Account**\n",
    "   - Visit [Kaggle](https://www.kaggle.com) and sign up for an account.\n",
    "\n",
    "2. **Obtain Kaggle API Key**\n",
    "   - Go to your Kaggle account settings.\n",
    "   - Find the \"API\" section and click on \"Create New API Token\".\n",
    "   - This will download a `kaggle.json` file containing your API key.\n",
    "\n",
    "3. **Install Kaggle Package**\n",
    "   - Use Conda to install the Kaggle package by running:\n",
    "     ```bash\n",
    "     conda install kaggle\n",
    "     ```\n",
    "\n",
    "4. **Configure API Key**\n",
    "   - Copy the `kaggle.json` file to your user directory under the `.kaggle` folder. On most systems, you can use the following command:\n",
    "     ```bash\n",
    "     mkdir -p ~/.kaggle\n",
    "     cp path_to_downloaded_kaggle.json ~/.kaggle/kaggle.json\n",
    "     chmod 600 ~/.kaggle/kaggle.json\n",
    "     ```\n",
    "   - Ensure the `.kaggle` directory and the `kaggle.json` file have the proper permissions by setting:\n",
    "     ```bash\n",
    "     chmod 600 ~/.kaggle/kaggle.json\n",
    "     ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import kaggle\n",
    "import Marketing_Campaign as mc\n",
    "# Pre processing\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Scoring \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "\n",
    "# models \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.pipeline import make_pipeline as make_imbalance_pipeline\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/rodsaldanha/arketing-campaign\n"
     ]
    }
   ],
   "source": [
    "# Get the data using an API call\n",
    "kaggle.api.dataset_download_files('rodsaldanha/arketing-campaign', path='resources', unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv(\"./resources/marketing_campaign.csv\",delimiter=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis)\n",
    "We will revisit this. For now We want the rough draft of the model\n",
    "#\n",
    "During EDA\n",
    "\n",
    "Visualize the data using plots and graphs to understand distributions and relationships between variables.\n",
    "Calculate summary statistics to get a sense of the central tendencies and variability.\n",
    "Identify any correlations between variables that might influence model choices.\n",
    "Detect and treat missing values or outliers that could skew the results of your analysis.\n",
    "Explore the data's structure to inform feature selection and engineering, which are key to building effective machine learning models.\n",
    "\n",
    "# read any and all documentation you can find on your dataset to understand it better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Education</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>Dt_Customer</th>\n",
       "      <th>Recency</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>...</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "      <th>AcceptedCmp3</th>\n",
       "      <th>AcceptedCmp4</th>\n",
       "      <th>AcceptedCmp5</th>\n",
       "      <th>AcceptedCmp1</th>\n",
       "      <th>AcceptedCmp2</th>\n",
       "      <th>Complain</th>\n",
       "      <th>Z_CostContact</th>\n",
       "      <th>Z_Revenue</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5524</td>\n",
       "      <td>1957</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Single</td>\n",
       "      <td>58138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-09-04</td>\n",
       "      <td>58</td>\n",
       "      <td>635</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2174</td>\n",
       "      <td>1954</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Single</td>\n",
       "      <td>46344.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-03-08</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4141</td>\n",
       "      <td>1965</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Together</td>\n",
       "      <td>71613.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-08-21</td>\n",
       "      <td>26</td>\n",
       "      <td>426</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6182</td>\n",
       "      <td>1984</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Together</td>\n",
       "      <td>26646.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-02-10</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5324</td>\n",
       "      <td>1981</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Married</td>\n",
       "      <td>58293.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-19</td>\n",
       "      <td>94</td>\n",
       "      <td>173</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  Year_Birth   Education Marital_Status   Income  Kidhome  Teenhome  \\\n",
       "0  5524        1957  Graduation         Single  58138.0        0         0   \n",
       "1  2174        1954  Graduation         Single  46344.0        1         1   \n",
       "2  4141        1965  Graduation       Together  71613.0        0         0   \n",
       "3  6182        1984  Graduation       Together  26646.0        1         0   \n",
       "4  5324        1981         PhD        Married  58293.0        1         0   \n",
       "\n",
       "  Dt_Customer  Recency  MntWines  ...  NumWebVisitsMonth  AcceptedCmp3  \\\n",
       "0  2012-09-04       58       635  ...                  7             0   \n",
       "1  2014-03-08       38        11  ...                  5             0   \n",
       "2  2013-08-21       26       426  ...                  4             0   \n",
       "3  2014-02-10       26        11  ...                  6             0   \n",
       "4  2014-01-19       94       173  ...                  5             0   \n",
       "\n",
       "   AcceptedCmp4  AcceptedCmp5  AcceptedCmp1  AcceptedCmp2  Complain  \\\n",
       "0             0             0             0             0         0   \n",
       "1             0             0             0             0         0   \n",
       "2             0             0             0             0         0   \n",
       "3             0             0             0             0         0   \n",
       "4             0             0             0             0         0   \n",
       "\n",
       "   Z_CostContact  Z_Revenue  Response  \n",
       "0              3         11         1  \n",
       "1              3         11         0  \n",
       "2              3         11         0  \n",
       "3              3         11         0  \n",
       "4              3         11         0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2240, 29)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display (data.head())\n",
    "# what does our data look like? At this point also use any documentation on the data set to find out what each value means and how it might be used is solving the business problem\n",
    "print (f'{data.shape}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop Percent of the rows is %2\n",
      "If the number of NA values in a column is less than the calculated threshold, automatically drop the NA rows.\n",
      "{'Income': 24}\n",
      "Automatically dropping rows in Income where NA values are present.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess = Clean up NA if number of NA in column is less that a percentage of rows\n",
    "# this automatically cleans up rows below a threshold and list columns when NA rows exceed the threshold\n",
    "data1=mc.auto_drop_na(data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that are not numeric :\n",
      " ['Education', 'Marital_Status', 'Dt_Customer']\n",
      "\n",
      "Education\n",
      "Graduation    1116\n",
      "PhD            481\n",
      "Master         365\n",
      "2n Cycle       200\n",
      "Basic           54\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Marital_Status\n",
      "Married     857\n",
      "Together    573\n",
      "Single      471\n",
      "Divorced    232\n",
      "Widow        76\n",
      "Alone         3\n",
      "Absurd        2\n",
      "YOLO          2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Dt_Customer\n",
      "2012-08-31    12\n",
      "2012-09-12    11\n",
      "2013-02-14    11\n",
      "2014-05-12    11\n",
      "2013-08-20    10\n",
      "              ..\n",
      "2012-08-05     1\n",
      "2012-11-18     1\n",
      "2014-05-09     1\n",
      "2013-06-26     1\n",
      "2014-01-09     1\n",
      "Name: count, Length: 662, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify non numeric columns we will need to deal with\n",
    "non_numeric= (data1.dtypes[(data1.dtypes != 'int64') & (data1.dtypes != 'float64')]).index.tolist()\n",
    "# display (data.dtypes)\n",
    "print (f'Columns that are not numeric :\\n {non_numeric}\\n')\n",
    "\n",
    "for column in non_numeric:\n",
    "    print (data1[column].value_counts())\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric data to group\n",
    "The year of birth data creates too many unique data to be useful. We have chosen to convert the Year Birth data to a 6 generation values. After one hot encode for generation we drop the year birth column. \n",
    "\n",
    "# non numeric column \n",
    "- Education OrdinalEncoder because education counts\n",
    "     - 0 - **Basic** This generally refers to elementary or primary education.\n",
    "     - 1 - **2n Cycle** This is not a commonly used term globally but might refer to secondary education or an intermediary level in some education systems.\n",
    "     - 2 - **Graduation** Typically refers to the completion of a bachelor's or undergraduate degree.\n",
    "     - 3 - **Master** A postgraduate degree that follows the completion of a bachelor's degree.\n",
    "     - 4 - **PhD** The highest university degree, typically following a master's degree.\n",
    "\n",
    "- Marital_Status - The status has no weighted values. change none standard answers to single and then one hot encode three remaining values\n",
    "     - Alone     Single\n",
    "     - Absurd    Single\n",
    "     - YOLO      Single\n",
    "\n",
    "- Year_Birth - Convert to 6 generations and then One Hot Encode\n",
    "\n",
    "- Dt_Customer - We will convert to data time and represent this in number of months the cusomter has been with us\n",
    "\n",
    "# ID column\n",
    "The id column is etiher a uniwue customer id or an index falue that became a column in the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education encode\n",
    "categories = ['Basic','2n Cycle','Graduation','Master','PhD']\n",
    "column = 'Education'\n",
    "data2 = mc.preprocess_ord(data1,column,categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot ENncode\n",
    "We want to One hot encode Marital status and generations.\n",
    "This takes some preperation\n",
    "-- Marital needs to change Alone, Absurd and YOLO to Single. \n",
    "-- Year_Birth needs to be encoded as generations first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marital Status Encode\n",
    "data2['Marital_Status'] = data2['Marital_Status'].replace(['Alone', 'Absurd', 'YOLO'], 'Single')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform Year_Birth to Generations\n",
    "data3 = mc.set_gen(data2,'Year_Birth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = mc.date_to_months(data3,'Dt_Customer',2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_column_list = ['Marital_Status', 'Generations']\n",
    "data5 = mc.preprocess_ohe(data4,ohe_column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with only one unique value\n",
    "single_value_columns = data5.nunique() == 1\n",
    "# Drop these columns from the DataFrame\n",
    "data5 = data5.loc[:, ~single_value_columns]\n",
    "data6 = data5.drop(['ID','Year_Birth'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recency                          -0.199766\n",
      "Teenhome                         -0.153901\n",
      "Marital_Status_Married           -0.079821\n",
      "Kidhome                          -0.077909\n",
      "Marital_Status_Together          -0.075293\n",
      "Generations_Generation X         -0.036497\n",
      "Generations_Unknown Generation   -0.012639\n",
      "Generations_Lost Generation      -0.008935\n",
      "NumWebVisitsMonth                -0.002209\n",
      "Complain                         -0.002029\n",
      "NumDealsPurchases                 0.003451\n",
      "Generations_Silent Generation     0.019236\n",
      "NumStorePurchases                 0.036241\n",
      "Marital_Status_Widow              0.045656\n",
      "Generations_Millennials           0.063313\n",
      "Education                         0.099150\n",
      "MntFishProducts                   0.108145\n",
      "Marital_Status_Single             0.114126\n",
      "MntSweetProducts                  0.116170\n",
      "MntFruits                         0.122443\n",
      "Income                            0.133047\n",
      "MntGoldProds                      0.140332\n",
      "NumWebPurchases                   0.151431\n",
      "AcceptedCmp2                      0.169294\n",
      "AcceptedCmp4                      0.180205\n",
      "Dt_Customer                       0.196302\n",
      "NumCatalogPurchases               0.219914\n",
      "MntMeatProducts                   0.237746\n",
      "MntWines                          0.246299\n",
      "AcceptedCmp3                      0.254005\n",
      "AcceptedCmp1                      0.297345\n",
      "AcceptedCmp5                      0.323374\n",
      "Response                          1.000000\n",
      "Name: Response, dtype: float64\n",
      "Features perfectly correlated with Y: Response    1.0\n",
      "Name: Response, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "correlations = data6.corr() \n",
    "\n",
    "\n",
    "# Check correlations with the target variable\n",
    "target_correlation = correlations['Response'].sort_values()\n",
    "\n",
    "# Display correlations\n",
    "print(target_correlation)\n",
    "\n",
    "# Look for any feature with a perfect correlation\n",
    "perfect_correlations = target_correlation[(target_correlation == 1) | (target_correlation == -1)]\n",
    "print(\"Features perfectly correlated with Y:\", perfect_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this section will address the data imbalance we see in our y value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (data6.head())\n",
    "print (data6.columns)\n",
    "y = data6['Response']\n",
    "# Checking the distribution of classes\n",
    "class_distribution = y.value_counts()\n",
    "print(class_distribution)\n",
    "# Getting the percentage distribution of the classes\n",
    "class_percentage = y.value_counts(normalize=True) * 100\n",
    "print(class_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of classes\n",
    "sns.countplot(x=data6['Response'])\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data6.drop(['Response'],axis=1)\n",
    "y=data6['Response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# life is balance in the fast lane\n",
    "\n",
    "Using \n",
    "\n",
    "classification report for\n",
    "- dataset with no changes\n",
    "- dataset with underfit RandomUnderSampler\n",
    "- dataset with overfit RandomOverSampler\n",
    "- dataset with smote SMOTE\n",
    "- dataset with smoteenn SMOTEENN\n",
    "\n",
    "Standard Scaler\n",
    "\n",
    "RandomForestClassifier\n",
    "\n",
    "no results are accepable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# List of classifiers\n",
    "classifiers = [\n",
    "    ('RFC', RandomForestClassifier(random_state=42)),\n",
    "    ('GBC', GradientBoostingClassifier(random_state=42)),\n",
    "    ('KNN', KNeighborsClassifier()),\n",
    "    ('SVC', SVC(random_state=42)),\n",
    "    ('LR', LogisticRegression(max_iter=10000)),\n",
    "    ('DTC', DecisionTreeClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# List of scalers\n",
    "scalers = [\n",
    "    ('SS', StandardScaler()),\n",
    "    ('MMS', MinMaxScaler())\n",
    "]\n",
    "\n",
    "# List of resampling methods\n",
    "samplers = [\n",
    "    ('RUS', RandomUnderSampler(random_state=42)),\n",
    "    ('ROS', RandomOverSampler(random_state=42)),\n",
    "    ('SMOTE', SMOTE(random_state=42)),\n",
    "    ('SMOTEENN', SMOTEENN(random_state=42))\n",
    "]\n",
    "\n",
    "# Evaluating each combination of sampler, scaler, and classifier\n",
    "results = []\n",
    "for sampler_name, sampler in samplers:\n",
    "    for scaler_name, scaler in scalers:\n",
    "        for classifier_name, classifier in classifiers:\n",
    "            pipeline = make_imbalance_pipeline(sampler, scaler, classifier)\n",
    "            cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "            scores = cross_val_score(pipeline, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "            results.append((sampler_name, scaler_name, classifier_name, scores.mean()))\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results, columns=['Sampler', 'Scaler', 'Classifier', 'Accuracy'])\n",
    "sorted_results_df = results_df.sort_values(by='Accuracy', ascending=False)\n",
    "print(sorted_results_df.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train, y_train are your training data\n",
    "scores = []\n",
    "estimators_range = range(5, 100, 5)  # From 10 to 300 in steps of 20\n",
    "\n",
    "for n in estimators_range:\n",
    "    model = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')  # 5-fold cross-validation\n",
    "    scores.append(np.mean(score))\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(estimators_range, scores, marker='o')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effect of n_estimators')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Identify features with less that 1% importance and drop them\n",
    "#  \n",
    "model = RandomForestClassifier(n_estimators=75,random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances and create a mask for dropping features\n",
    "importances = model.feature_importances_\n",
    "features_to_drop = importances < 0.01\n",
    "\n",
    "# Drop features from DataFrame\n",
    "X_reduced = X.loc[:, ~features_to_drop]\n",
    "\n",
    "print(f\"Original number of features: {X.shape[1]}\")\n",
    "print(f\"Reduced number of features: {X_reduced.shape[1]}\")\n",
    "X=X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the original data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Step 2: Address imbalance with RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# Step 3: Scale the data (after resampling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)  # Fit and transform training data\n",
    "X_test_scaled = scaler.transform(X_test)  # Transform test data based on training scaler\n",
    "\n",
    "# Step 4: Train and evaluate the model\n",
    "model = RandomForestClassifier(n_estimators=75,random_state=42)\n",
    "model.fit(X_train_scaled, y_train_resampled)  # Use the scaled and resampled training data\n",
    "y_pred = model.predict(X_test_scaled)  # Predict using the scaled test data\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print (y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X['Intercept'] = 1  \n",
    "\n",
    "vif_data_df = pd.DataFrame()\n",
    "vif_data_df[\"Feature\"] = X.columns\n",
    "vif_data_df[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "sorted_vif_data_df = vif_data_df.sort_values(by='VIF', ascending=False)\n",
    "print(sorted_vif_data_df)\n",
    "\n",
    "\n",
    "# Calculate VIF for each variable\n",
    "# vif_data_df[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "# vif_data_df[\"Variable\"] = X.columns\n",
    "\n",
    "# Reviewing high VIF values\n",
    "# print(vif_data_df[vif_data_df['VIF'] == float('inf')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns for VIF calculation\n",
    "X = data6.select_dtypes(include=[np.number])  \n",
    "\n",
    "# It's common practice to add an intercept for regression models, but for VIF calculation\n",
    "# it's typically not necessary unless you're explicitly modeling the intercept\n",
    "# X['Intercept'] = 1  \n",
    "\n",
    "# Initialize DataFrame to store VIF values\n",
    "vif_data_df = pd.DataFrame()\n",
    "vif_data_df[\"Feature\"] = X.columns  # Assign column names first\n",
    "\n",
    "# Calculate VIF for each variable and store in DataFrame\n",
    "vif_data_df[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Sort the DataFrame by VIF values in descending order\n",
    "sorted_vif_data_df = vif_data_df.sort_values(by='VIF', ascending=False)\n",
    "\n",
    "# Print the sorted VIF values\n",
    "print(sorted_vif_data_df)\n",
    "\n",
    "# Optional: Print DataFrame to check data (unrelated to VIF calculation)\n",
    "print(data6.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)  # Ensure X and y are your feature matrix and target vector respectively\n",
    "importance = model.feature_importances_\n",
    "\n",
    "# Correct printing method\n",
    "for i, v in enumerate(importance):\n",
    "    print('Feature: %d, Score: %.5f' % (i, v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
