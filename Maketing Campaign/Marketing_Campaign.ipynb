{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and setup kaggle and api key\n",
    "- get a kaggle account\n",
    "- Get Kaggle API\n",
    "- conda install kaggle\n",
    "- copy the kaggle.json file from download to your user directory/.kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data using an API call\n",
    "kaggle.api.dataset_download_files('rodsaldanha/arketing-campaign', path='resources', unzip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv(\"./resources/marketing_campaign.csv\",delimiter=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis)\n",
    "We will revisit this. For now We want the rough draft of the model\n",
    "#\n",
    "During EDA\n",
    "\n",
    "Visualize the data using plots and graphs to understand distributions and relationships between variables.\n",
    "Calculate summary statistics to get a sense of the central tendencies and variability.\n",
    "Identify any correlations between variables that might influence model choices.\n",
    "Detect and treat missing values or outliers that could skew the results of your analysis.\n",
    "Explore the data's structure to inform feature selection and engineering, which are key to building effective machine learning models.\n",
    "\n",
    "# read any and all documentation you can find on your dataset to understand it better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display (data.head())\n",
    "# what does our data look like? At this point also use any documentation on the data set to find out what each value means and how it might be used is solving the business problem\n",
    "display (data.shape)\n",
    "print (f'Columns with NA valuses \\n {data.isna().sum()[lambda x: x > 0]}')\n",
    "# Make desision about null values. Can we fill them of should we drop rows with null values?\n",
    "non_numeric= (data.dtypes[(data.dtypes != 'int64') & (data.dtypes != 'float64')]).index.tolist()\n",
    "# display (data.dtypes)\n",
    "print (f'Columns that are not numeric :\\n {non_numeric}')\n",
    "# Explore non numberic type to see how we can use them in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to get started we will drop NA and columns that are not numberic. this will let us get a rough model\n",
    "# we come back to this and preprocess based on the draft results if needed\n",
    "\n",
    "\n",
    "data_drop_columns = data.drop(columns=non_numeric, axis=1)\n",
    "data_drop_na = data_drop_columns.dropna()\n",
    "df = data_drop_na.copy()\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Train and Test **80/20 split**\n",
    "# add verbage as to why we picked response\n",
    "\n",
    "X = df.drop('Response', axis=1)\n",
    "y = df[\"Response\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "# This will split 'X' and 'y' such that 80% is used for training and 20% is used for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the data evens out the high and low values so on one column dominates over all others\n",
    "We will want to compare the scores of standard scalar to Min Max scalar to pick the bast scaling methood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the X data by using StandardScaler()\n",
    "#\n",
    "scaler_ss= StandardScaler().fit(X_train)\n",
    "X_train_ss_scaled = scaler_ss.transform(X_train)\n",
    "display (X_train_ss_scaled)\n",
    "# Transform the test dataset based on the fit from the training dataset\n",
    "X_test_ss_scaled = scaler_ss.transform(X_test)\n",
    "display (X_test_ss_scaled)\n",
    "#\n",
    "# Create a `LogisticRegression` function and assign it \n",
    "# to a variable named `logistic_regression_model`.\n",
    "logistic_regression_model_ss = LogisticRegression()\n",
    "# Fit the model\n",
    "logistic_regression_model_ss.fit(X_train_ss_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets look at min max scaler\n",
    "scaler_mm = MinMaxScaler().fit(X_train)\n",
    "X_train_mm_scaled = scaler_mm.transform(X_train)\n",
    "display (X_train_mm_scaled)\n",
    "#\n",
    "X_test_mm_scaled = scaler_mm.transform(X_test)\n",
    "display (X_test_mm_scaled)\n",
    "\n",
    "# Create a `LogisticRegression` function and assign it \n",
    "# to a variable named `logistic_regression_model`.\n",
    "logistic_regression_model_mm = LogisticRegression()\n",
    "# Fit the model\n",
    "logistic_regression_model_mm.fit(X_train_mm_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Scaler\n",
      "Training Data Score: 0.8871331828442438\n",
      "Testing Data Score: 0.8738738738738738\n",
      "Min Max Scaler\n",
      "Training Data Score: 0.8905191873589164\n",
      "Testing Data Score: 0.8738738738738738\n"
     ]
    }
   ],
   "source": [
    "# Score the model\n",
    "\n",
    "print(f\"Standard Scaler\\nTraining Data Score: {logistic_regression_model_ss.score(X_train_ss_scaled, y_train)}\")\n",
    "print(f\"Testing Data Score: {logistic_regression_model_ss.score(X_test_ss_scaled, y_test)}\")\n",
    "print(f\"Min Max Scaler\\nTraining Data Score: {logistic_regression_model_mm.score(X_train_mm_scaled, y_train)}\")\n",
    "print(f\"Testing Data Score: {logistic_regression_model_mm.score(X_test_mm_scaled, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
